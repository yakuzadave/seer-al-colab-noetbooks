{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-ML-HF-Testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yakuzadave/seer-al-colab-noetbooks/blob/main/Colab_ML_HF_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n6CYTIja4hq"
      },
      "source": [
        "To start this Jupyter Dash app, please run all the cells below. Then, click on the **temporary** URL at the end of the last cell to open the app."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPEkJcCxSqxs"
      },
      "source": [
        "!pip install -q jupyter-dash dash plotly slack_sdk rich gspread d20 dash-bootstrap-components transformers datasets"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNQanZGJcJhx"
      },
      "source": [
        "import time\n",
        "import google.colab\n",
        "import dash\n",
        "from dash import html\n",
        "google.colab.output.serve_kernel_port_as_iframe\n",
        "from dash import dcc\n",
        "import dash_bootstrap_components as dbc\n",
        "from dash.dependencies import Input, Output, State\n",
        "from jupyter_dash import JupyterDash\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModelForCausalLM, GPTNeoForCausalLM, GPT2Tokenizer, GPT2PreTrainedModel\n",
        "import torch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmYDDzbccMX0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07cec710-c815-4b21-840f-5c0c5d041872"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "print(\"Start loading model...\")\n",
        "\n",
        "# # name = \"microsoft/DialoGPT-large\"\n",
        "name = \"gpt2\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(name)\\\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(name)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "\n",
        "# model = AutoModelWithLMHead.from_pretrained(name)\n",
        "# model = AutoModelForCausalLM.from_pretrained(name)\n",
        "# model = GPTNeoForCausalLM.from_pretrained(name)\n",
        "#model = AutoModelForCausalLM.from_pretrained(name)\n",
        "model = GPT2PreTrainedModel.from_pretrained(name)\n",
        "\n",
        "# Switch to cuda, eval mode, and FP16 for faster inference\n",
        "# if device == \"cuda\":\n",
        "#     model = model.half()\n",
        "# model.to(device)\n",
        "#odel.eval()\n",
        "\n",
        "print(\"Done.\")\n",
        "chat_history = \"\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Start loading model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at gpt2 were not used when initializing GPT2PreTrainedModel: ['h.2.attn.c_attn.bias', 'h.10.attn.c_attn.weight', 'h.6.ln_2.weight', 'h.0.ln_2.bias', 'h.6.attn.c_attn.bias', 'h.11.attn.c_proj.weight', 'h.10.ln_1.bias', 'h.8.mlp.c_proj.bias', 'h.8.mlp.c_fc.weight', 'h.8.attn.c_attn.bias', 'h.0.attn.c_attn.bias', 'h.4.attn.c_attn.weight', 'h.7.mlp.c_proj.bias', 'h.3.ln_2.weight', 'h.1.mlp.c_fc.bias', 'h.6.ln_2.bias', 'h.5.attn.c_proj.bias', 'h.11.ln_2.weight', 'h.1.attn.c_attn.bias', 'h.6.mlp.c_proj.bias', 'h.2.mlp.c_fc.bias', 'h.4.attn.c_attn.bias', 'h.4.ln_2.bias', 'h.3.ln_2.bias', 'h.7.mlp.c_proj.weight', 'h.4.ln_2.weight', 'h.7.ln_2.bias', 'h.2.mlp.c_fc.weight', 'h.8.ln_2.weight', 'h.1.ln_2.weight', 'h.9.attn.c_proj.bias', 'h.5.ln_2.bias', 'h.7.mlp.c_fc.weight', 'h.2.attn.c_attn.weight', 'h.10.mlp.c_fc.weight', 'h.5.ln_1.weight', 'h.9.attn.c_attn.weight', 'h.1.mlp.c_fc.weight', 'h.4.ln_1.bias', 'h.10.mlp.c_proj.weight', 'h.10.mlp.c_proj.bias', 'h.6.attn.c_attn.weight', 'h.9.attn.c_proj.weight', 'h.0.attn.bias', 'h.1.ln_1.weight', 'h.11.attn.c_proj.bias', 'h.0.mlp.c_proj.bias', 'h.5.attn.bias', 'h.0.ln_1.weight', 'h.6.ln_1.bias', 'h.9.ln_1.weight', 'h.3.ln_1.bias', 'h.9.ln_2.weight', 'h.10.mlp.c_fc.bias', 'h.2.attn.c_proj.bias', 'h.5.mlp.c_fc.bias', 'h.10.attn.bias', 'h.3.attn.c_attn.bias', 'h.9.attn.c_attn.bias', 'h.3.attn.bias', 'h.3.mlp.c_fc.bias', 'h.9.mlp.c_fc.bias', 'h.11.ln_1.weight', 'h.8.ln_1.weight', 'h.5.mlp.c_proj.weight', 'h.8.ln_1.bias', 'h.9.mlp.c_proj.bias', 'h.2.ln_1.weight', 'h.11.attn.c_attn.weight', 'h.3.mlp.c_proj.bias', 'h.3.attn.c_proj.weight', 'h.9.mlp.c_fc.weight', 'h.2.mlp.c_proj.weight', 'h.8.mlp.c_fc.bias', 'h.11.ln_1.bias', 'h.7.attn.c_proj.bias', 'h.11.mlp.c_proj.bias', 'h.5.ln_2.weight', 'h.3.attn.c_proj.bias', 'h.8.ln_2.bias', 'h.11.mlp.c_fc.weight', 'h.7.attn.c_proj.weight', 'h.5.attn.c_attn.weight', 'h.8.mlp.c_proj.weight', 'h.3.ln_1.weight', 'h.10.ln_1.weight', 'h.2.ln_2.bias', 'wpe.weight', 'h.0.attn.c_proj.bias', 'h.4.ln_1.weight', 'h.8.attn.c_proj.bias', 'h.7.attn.c_attn.weight', 'h.2.ln_2.weight', 'h.5.mlp.c_fc.weight', 'h.1.mlp.c_proj.weight', 'h.0.ln_1.bias', 'h.1.ln_2.bias', 'h.2.mlp.c_proj.bias', 'h.7.attn.c_attn.bias', 'h.11.mlp.c_fc.bias', 'h.9.attn.bias', 'h.6.ln_1.weight', 'h.2.attn.c_proj.weight', 'h.0.attn.c_proj.weight', 'h.1.attn.c_attn.weight', 'h.4.mlp.c_fc.bias', 'h.6.attn.c_proj.bias', 'h.4.attn.c_proj.bias', 'h.6.mlp.c_fc.weight', 'h.7.mlp.c_fc.bias', 'h.9.ln_1.bias', 'h.8.attn.bias', 'h.11.ln_2.bias', 'h.0.mlp.c_fc.weight', 'h.3.mlp.c_proj.weight', 'h.8.attn.c_attn.weight', 'h.10.ln_2.weight', 'h.9.ln_2.bias', 'h.4.attn.c_proj.weight', 'h.9.mlp.c_proj.weight', 'h.7.ln_1.weight', 'h.3.mlp.c_fc.weight', 'h.3.attn.c_attn.weight', 'h.10.ln_2.bias', 'h.4.mlp.c_fc.weight', 'h.11.attn.c_attn.bias', 'h.4.mlp.c_proj.bias', 'h.6.mlp.c_fc.bias', 'h.1.attn.c_proj.bias', 'h.8.attn.c_proj.weight', 'h.11.mlp.c_proj.weight', 'h.1.attn.c_proj.weight', 'h.4.attn.bias', 'h.6.attn.c_proj.weight', 'h.1.attn.bias', 'h.2.ln_1.bias', 'h.6.attn.bias', 'h.4.mlp.c_proj.weight', 'h.5.ln_1.bias', 'h.1.ln_1.bias', 'h.2.attn.bias', 'h.0.attn.c_attn.weight', 'h.5.attn.c_proj.weight', 'h.6.mlp.c_proj.weight', 'h.10.attn.c_attn.bias', 'h.0.mlp.c_fc.bias', 'h.10.attn.c_proj.weight', 'ln_f.bias', 'h.0.ln_2.weight', 'wte.weight', 'h.7.ln_1.bias', 'h.7.ln_2.weight', 'h.10.attn.c_proj.bias', 'h.0.mlp.c_proj.weight', 'h.5.attn.c_attn.bias', 'h.5.mlp.c_proj.bias', 'ln_f.weight', 'h.7.attn.bias', 'h.1.mlp.c_proj.bias', 'h.11.attn.bias']\n",
            "- This IS expected if you are initializing GPT2PreTrainedModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2PreTrainedModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title User Input\n",
        "#@markdown Input entered in the field below will be added in to the chat history for evaluation\n",
        "\n",
        "user_input = 'That is pretty interesting'  #@param {type: \"string\"}\n",
        "max_tokens = 2554  #@param {type: \"slider\", min: 100, max: 5000}\n",
        "\n",
        "\n",
        "#@markdown ---\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TpmgWDC0YS8B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n6HT0CIcWJ9"
      },
      "source": [
        "\n",
        "\n",
        "input_ids = tokenizer(f\"{chat_history}\\n{user_input}<|endoftext|>\", return_tensors=\"pt\")\n",
        "#input_ids = tokenizer(user_input, return_tensors=\"pt\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids['input_ids'], do_sample=True, max_length=max_tokens).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "BekBpx6JbLJG",
        "outputId": "c895d42e-a284-4a55-da48-6ff1b48d93c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7a0f803b7c01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m             )\n\u001b[1;32m   1322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1928\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1929\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1930\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1931\u001b[0m             )\n\u001b[1;32m   1932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: _forward_unimplemented() got an unexpected keyword argument 'input_ids'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgwQH_TXckgN"
      },
      "source": [
        "chat_history = tokenizer.batch_decode(outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[print(x) for x in chat_history]"
      ],
      "metadata": {
        "id": "0kn2v0GAF6jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vrRftDZlcxhv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}